{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Data Preprocessing 1: Filtereren en samenvoegen\n",
    "\n",
    "Student namen: Laiba Shamsul, Popke Snoek, Yoshi Fu, Pepeyn Velthuijse\n",
    "\n",
    "Team nummer: G4\n",
    "\n",
    "Hieronder is de code weergegeven dat is gebruikt om de verschillende datasets te filteren en te combineren. Merk op dat de code niet daadwerkelijk wordt uitgevoerd omdat het ongeveer 3 uur zou duren om te beëindigen. Dit komt door het aantal benodigde bewerkingen om *strings* in kolommen te filteren. Merk op dat de onderstaande code nooit is uitgevoerd. De gefilterde dataset is namelijk verkregen door 'clean.py' uit te voeren. Dit bestand is te vinden in de GitHub-repository (https://github.com/FuYoshi/data_story_project).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Specify parameters\n",
    "\n",
    "# Define chunk size to read the large csv files in chunks.\n",
    "chunk_size = 1000000\n",
    "\n",
    "# Define the original datasets that should be filtered and combined.\n",
    "csv_file_list = [\n",
    "    \"CO2_2015.csv\",\n",
    "    \"CO2_2016.csv\",\n",
    "    \"CO2_2017.csv\",\n",
    "    \"CO2_2018.csv\",\n",
    "    \"CO2_2019.csv\",\n",
    "    \"CO2_2020.csv\",\n",
    "    \"CO2_2021.csv\",\n",
    "]\n",
    "\n",
    "# Define the output file.\n",
    "output_file = \"CO2_data.csv\"\n",
    "\n",
    "# Remove rows in the datasets that have a missing value in the following columns.\n",
    "non_null_columns = [\"Country\", \"Mk\", \"Cn\", \"m (kg)\", \"W (mm)\", \"Ft\"]\n",
    "\n",
    "# Remove rows with the following values in the fuel type column.\n",
    "fuel_type_filter = [\"hydrogen\"]\n",
    "\n",
    "# Only keep the following well-known car brands.\n",
    "car_makes = [\n",
    "    \"Avtokad\", \"Aiways\", \"Alfa Romeo\", \"Alpina\", \"Alpine\", \"Ariel Atom\",\n",
    "    \"Aston Martin\", \"Audi\", \"BAIC\", \"Bentley\", \"Benimar\", \"BMW\", \"Brabus\",\n",
    "    \"Bugatti\", \"Buick\", \"BYD\", \"Cadillac\", \"Caterham\", \"Chevrolet\", \"Chrysler\",\n",
    "    \"Citroën\", \"CUPRA\", \"Dacia\", \"Daewoo\", \"DFSK\", \"Daf\", \"Daihatsu\",\n",
    "    \"Dallara\", \"Datsun\", \"DeLorean\", \"Dodge\", \"Donkervoort\", \"Dreamer\", \"DR\",\n",
    "    \"DS\", \"Ferrari\", \"Fiat\", \"Fisker\", \"Ford\", \"GINAF\", \"GMC\", \"Hino Motors\",\n",
    "    \"Honda\", \"Hummer\", \"Hyundai\", \"Infiniti\", \"IVECO\", \"JAC\", \"Jaguar\", \"Jeep\",\n",
    "    \"Karma\", \"KEAT\", \"Kia\", \"Koenigsegg\", \"KTM\", \"Lada\", \"Lamborghini\",\n",
    "    \"Land Rover\", \"Lancia\", \"Lexus\", \"Lynk & Co\", \"Lotus\", \"MAN\", \"Mahindra\",\n",
    "    \"Maserati\", \"Maxus\", \"Mazda\", \"McLaren\", \"Mercedes\", \"Mini\", \"Mitsubishi\",\n",
    "    \"Morgan\", \"Nissan\", \"Nismo\", \"Opel\", \"Pagani\", \"Peugeot\", \"Polestar\",\n",
    "    \"Porsche\", \"Puma\", \"Renault\", \"Rolls Royce\", \"Rover\", \"Range Rover\",\n",
    "    \"Saab\", \"Scania\", \"Scion\", \"Seat\", \"Škoda\", \"Skywell\", \"Smart\",\n",
    "    \"SsangYong\", \"Subaru\", \"Suzuki\", \"Tesla\", \"Toyota\", \"Tripod\", \"UAZ\",\n",
    "    \"Volkswagen\", \"Volvo\", \"ZD\",\n",
    "]\n",
    "\n",
    "# Specify synonyms that car brands might use in the dataset (i.e. VW instead of Volkswagen).\n",
    "car_synonyms = {\n",
    "    \"VW\": \"Volkswagen\",\n",
    "    \"Citroen\": \"Citroën\",\n",
    "    \"Skoda\": \"Skoda\",\n",
    "    \"Mazda\": \"placeholder1\",\n",
    "    \"Dreamer\": \"placeholder2\",\n",
    "}\n",
    "\n",
    "# Define placeholders for car brands that are a subset of another brand (i.e. DR is a subset of DREAMER).\n",
    "car_placeholders = {\n",
    "    \"placeholder1\": \"Mazda\",\n",
    "    \"placeholder2\": \"Dreamer\",\n",
    "}\n",
    "\n",
    "# Define the column names that the original datasets use.\n",
    "COLS1 = [\"MS\", \"Mk\", \"Cn\", \"m (kg)\", \"e (g/km)\", \"w (mm)\", \"Ft\", \"Er (g/km)\"]\n",
    "COLS2 = [\"MS\", \"Mk\", \"Cn\", \"m (kg)\", \"Enedc (g/km)\", \"Ewltp (g/km)\", \"W (mm)\", \"Ft\", \"Ernedc (g/km)\", \"Erwltp (g/km)\"]\n",
    "COLS3 = [\"Country\", \"Mk\", \"Cn\", \"m (kg)\", \"Enedc (g/km)\", \"Ewltp (g/km)\", \"W (mm)\", \"Ft\", \"Ernedc (g/km)\", \"Erwltp (g/km)\"]\n",
    "TYPE1 = {\"MS\": str, \"Mk\": str, \"Cn\": str, \"m (kg)\": float, \"e (g/km)\": float, \"w (mm)\": float, \"Ft\": str, \"Er (g/km)\": float}\n",
    "TYPE2 = {\"MS\": str, \"Mk\": str, \"Cn\": str, \"m (kg)\": float, \"Enedc (g/km)\": float, \"Ewltp (g/km)\": float, \"W (mm)\": float, \"Ft\": str, \"Ernedc (g/km)\": float, \"Erwltp (g/km)\": float}\n",
    "TYPE3 = {\"Country\": str, \"Mk\": str, \"Cn\": str, \"m (kg)\": float, \"Enedc (g/km)\": float, \"Ewltp (g/km)\": float, \"W (mm)\": float, \"Ft\": str, \"Ernedc (g/km)\": float, \"Erwltp (g/km)\": float}\n",
    "RENAME1 = {\"MS\": \"Country\", \"e (g/km)\": \"Enedc (g/km)\", \"w (mm)\": \"W (mm)\", \"Er (g/km)\": \"Ernedc (g/km)\"}\n",
    "RENAME2 = {\"MS\": \"Country\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Read the csv files using the correct parameters.\n",
    "def _read_csv(csv_filename: str):\n",
    "    \"\"\"\n",
    "    Read the csv files. Since the keys changed throughout the years, the\n",
    "    arguments are set based on the year.\n",
    "    \"\"\"\n",
    "    # Configure the arguments of read_csv.\n",
    "    if \"2015\" in csv_filename:\n",
    "        args = {\"usecols\": COLS1, \"dtype\": TYPE1, \"encoding\": None, \"sep\": '\\t', \"rename\": RENAME1, \"year\": 2015}\n",
    "    elif \"2016\" in csv_filename:\n",
    "        args = {\"usecols\": COLS1, \"dtype\": TYPE1, \"encoding\": \"utf-16\", \"sep\": '\\t', \"rename\": RENAME1, \"year\": 2016}\n",
    "    elif \"2017\" in csv_filename:\n",
    "        args = {\"usecols\": COLS2, \"dtype\": TYPE2, \"encoding\": \"utf-16\", \"sep\": '\\t', \"rename\": RENAME2, \"year\": 2017}\n",
    "    elif \"2018\" in csv_filename:\n",
    "        args = {\"usecols\": COLS2, \"dtype\": TYPE2, \"encoding\": None, \"sep\": '\\t', \"rename\": RENAME2,  \"year\": 2018}\n",
    "    elif \"2019\" in csv_filename:\n",
    "        args = {\"usecols\": COLS3, \"dtype\": TYPE3, \"encoding\": None, \"sep\": ',', \"rename\": None,  \"year\": 2019}\n",
    "    elif \"2020\" in csv_filename:\n",
    "        args = {\"usecols\": COLS3, \"dtype\": TYPE3, \"encoding\": None, \"sep\": ',', \"rename\": None,  \"year\": 2020}\n",
    "    elif \"2021\" in csv_filename:\n",
    "        args = {\"usecols\": COLS3, \"dtype\": TYPE3, \"encoding\": None, \"sep\": ',', \"rename\": None,  \"year\": 2021}\n",
    "\n",
    "    # Read the csv using the arguments.\n",
    "    chunk_container = pd.read_csv(\n",
    "        csv_filename,\n",
    "        usecols=args[\"usecols\"],\n",
    "        dtype=args[\"dtype\"],\n",
    "        chunksize=chunk_size,\n",
    "        sep=args[\"sep\"],\n",
    "        encoding=args[\"encoding\"],\n",
    "    )\n",
    "\n",
    "    for chunk in chunk_container:\n",
    "        # Add missing columns.\n",
    "        if args[\"year\"] == 2015 or args[\"year\"] == 2016:\n",
    "            chunk[\"Ewltp (g/km)\"] = np.nan\n",
    "            chunk[\"Erwltp (g/km)\"] = np.nan\n",
    "        # Add year column.\n",
    "        chunk[\"year\"] = args[\"year\"]\n",
    "        if args[\"rename\"]:\n",
    "            chunk = chunk.rename(columns=args[\"rename\"])\n",
    "        # Sort columns lexicographically for consistency.\n",
    "        yield chunk[sorted(chunk.columns)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Filter de datasets and make the column values more consistent.\n",
    "def _filter(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Filter out records with missing values.\n",
    "    Filter out records with hydrogen as fuel type.\n",
    "    Make fuel type naming convention consistent.\n",
    "    Make make naming convention consistent.\n",
    "    Make commercial name naming convention consistent.\n",
    "    \"\"\"\n",
    "    df = _filter_Ft(df)\n",
    "    df = _filter_Mk(df)\n",
    "    df = _filter_Cn(df)\n",
    "    df = _convert_nedc(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def _filter_Ft(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Make the naming convention of the fuel type column consistent.\n",
    "    \"\"\"\n",
    "    # Ensure consistent naming convention in fuel types.\n",
    "    df[\"Ft\"] = df[\"Ft\"].str.lower().str.replace('/', '-').str.replace(\"unknown\", \"other\").str.strip()\n",
    "\n",
    "    # Filter out hydrogen and records with missing values.\n",
    "    df = df[~df[\"Ft\"].isin(fuel_type_filter)]\n",
    "    df = df[df[non_null_columns].notnull().all('columns')]\n",
    "    return df\n",
    "\n",
    "\n",
    "def _filter_Mk(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Make the naming convention of the make column consistent.\n",
    "    \"\"\"\n",
    "    for make in car_makes:\n",
    "        # Change synonyms that might have been used (i.e. \"VW\").\n",
    "        for k, v in car_synonyms.items():\n",
    "            df.loc[df[\"Mk\"].str.contains(k, case=False), \"Mk\"] = v\n",
    "\n",
    "        # Replace str with its brand (i.e. \"Mercedes-Benz\" -> \"Mercedes\").\n",
    "        compare_make = make.strip().replace('-', '').replace(' ', '')\n",
    "        df.loc[df[\"Mk\"].str.strip().str.replace('-', '').str.replace(' ', '').str.contains(compare_make, case=False), \"Mk\"] = make\n",
    "\n",
    "    # Convert placeholders back to their brand. This is because when a brand\n",
    "    # contains the string \"dreamer\", it automatically also contains the string\n",
    "    # \"DR\". Which is another brand.\n",
    "    for k, v in car_placeholders.items():\n",
    "        df.loc[df[\"Mk\"].str.contains(k, case=False), \"Mk\"] = v\n",
    "\n",
    "    # Remove everything that is not in the predefined brands.\n",
    "    df = df[df[\"Mk\"].isin(car_makes)]\n",
    "    return df\n",
    "\n",
    "\n",
    "def _filter_Cn(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Convert commercial names to uppercase and strip them.\n",
    "    \"\"\"\n",
    "    df[\"Cn\"] = df[\"Cn\"].str.upper().str.strip()\n",
    "    return df\n",
    "\n",
    "def _convert_nedc(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    If there is no WLTP data, convert NEDC to WLTP using conversion factor.\n",
    "\n",
    "    Note that the new value is an estimation and not absolute.\n",
    "    \"\"\"\n",
    "    conversion_factor = 1.3\n",
    "    newcol = df[\"Enedc (g/km)\"] * conversion_factor\n",
    "    df[\"Ewltp (g/km)\"] = newcol.where(df[\"Ewltp (g/km)\"].isna(), other=df['Ewltp (g/km)'])\n",
    "    newcol = df[\"Ernedc (g/km)\"] * conversion_factor\n",
    "    df[\"Erwltp (g/km)\"] = newcol.where(df[\"Erwltp (g/km)\"].isna(), other=df['Erwltp (g/km)'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Save the resulting dataset into file. Since the original datasets are so big,\n",
    "# the data of each chunk has to be written seperately.\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Read the csv files and filter them. Append the to the output file.\n",
    "    \"\"\"\n",
    "    append_header = True\n",
    "\n",
    "    # Iterate over each csv file to filter and merge.\n",
    "    for csv_filename in csv_file_list:\n",
    "        for chunk in _read_csv(csv_filename):\n",
    "            # Filter the given chunk.\n",
    "            chunk = _filter(chunk)\n",
    "\n",
    "            # Write the filtered chunk to the output file.\n",
    "            if append_header:\n",
    "                chunk.to_csv(output_file, mode=\"a\", index=False)\n",
    "                append_header = False\n",
    "            else:\n",
    "                chunk.to_csv(output_file, mode=\"a\", index=False, header=False)\n",
    "        print(f\"Finished csv {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Actually execute the above code. This is disabled as it would take approximately 3 hours.\n",
    "# # This is due to the amount of operations required for each column value.\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
